{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Agregar el directorio raiz al PYTHONPATH\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73536 entries, 0 to 73535\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      73536 non-null  object\n",
      " 1   polarity  73536 non-null  object\n",
      " 2   emotion   73536 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.7+ MB\n",
      "None\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8812 entries, 0 to 8811\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      8812 non-null   object\n",
      " 1   polarity  8812 non-null   object\n",
      " 2   emotion   8812 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 206.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from config.constants_tiktok import TIKTOK_DATASET_SENTENCES, TIKTOK_DATASET_TEXT\n",
    "df_sentences = pd.read_csv(TIKTOK_DATASET_SENTENCES)\n",
    "df_text = pd.read_csv(TIKTOK_DATASET_TEXT)\n",
    "print(df_sentences.info())\n",
    "print()\n",
    "print(df_text.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.models.w2v import create_w2v_model, get_model\n",
    "from src.nlp.nlp_spacy import tokenize\n",
    "from config.constants_models import EMBEDDING_W2V_TIKTOK_TEXT_PATH, EMBEDDING_W2V_TIKTOK_SENTENCES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir dimensiÃ³n de los embeddings\n",
    "\n",
    "DIM_EMBEDDINGS = 300\n",
    "SG=1\n",
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar modelo de embeddings con W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8812/8812 [02:08<00:00, 68.68it/s] \n",
      "8812it [02:08, 68.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo de W2V...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8812/8812 [00:00<00:00, 30039.94it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EMBEDDING_W2V_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# tokenizar el texto\u001b[39;00m\n\u001b[0;32m      5\u001b[0m docs \u001b[38;5;241m=\u001b[39m tokenize(df_text[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[43mcreate_w2v_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDIM_EMBEDDINGS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mSG\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Diego\\Desktop\\sentiment_analysis_pt\\src\\models\\w2v.py:17\u001b[0m, in \u001b[0;36mcreate_w2v_model\u001b[1;34m(docs, dim_embeddings, epochs, sg)\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(\n\u001b[0;32m      9\u001b[0m     sentences\u001b[38;5;241m=\u001b[39mdocs,\n\u001b[0;32m     10\u001b[0m     vector_size\u001b[38;5;241m=\u001b[39mdim_embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     sg\u001b[38;5;241m=\u001b[39msg \u001b[38;5;66;03m# skipgram or cbow\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39msave(EMBEDDING_W2V_TIKTOK_TEXT_PATH)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings guardados en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMBEDDING_W2V_TIKTOK_TEXT_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EMBEDDING_W2V_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenizar el texto\n",
    "\n",
    "\n",
    "docs = tokenize(df_text['text'])\n",
    "create_w2v_model(\n",
    "    docs,\n",
    "    DIM_EMBEDDINGS,\n",
    "    EPOCHS,\n",
    "    SG,\n",
    "    EMBEDDING_W2V_TIKTOK_TEXT_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test w2v model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TamaÃ±o del vocabulario: 13740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ipn', 0.4461907744407654),\n",
       " ('universidad', 0.4412895739078522),\n",
       " ('estudiar', 0.435004323720932),\n",
       " ('ðŸ’™', 0.40884000062942505),\n",
       " ('facultad', 0.40190833806991577),\n",
       " ('ðŸŽ“', 0.38928669691085815),\n",
       " ('admision', 0.3817272484302521),\n",
       " ('ðŸ’›', 0.3763902485370636),\n",
       " ('examen', 0.3731352984905243),\n",
       " ('medicina', 0.3723740875720978)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar modelo\n",
    "w2v = get_model(EMBEDDING_W2V_TIKTOK_TEXT_PATH)\n",
    "\n",
    "vocabulario = list(w2v.wv.index_to_key)\n",
    "print(f\" TamaÃ±o del vocabulario: {len(vocabulario)}\")\n",
    "\n",
    "w2v.wv.most_similar(\"unam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enojado', 0.37608930468559265),\n",
       " ('cumpleaÃ±os', 0.36935099959373474),\n",
       " ('fingiendo', 0.36420661211013794),\n",
       " ('estresado', 0.33905744552612305),\n",
       " ('orgullosa', 0.32536694407463074),\n",
       " ('dormida', 0.3193485736846924),\n",
       " ('respondÃ­', 0.3172473907470703),\n",
       " ('reclamando', 0.3132553994655609),\n",
       " ('comiendo', 0.31319254636764526),\n",
       " ('soÃ±ando', 0.3121330738067627)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(\"feliz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73536/73536 [02:28<00:00, 495.41it/s]\n",
      "73536it [02:28, 495.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo de W2V...\n",
      "Embeddings guardados en c:\\Users\\Diego\\Desktop\\sentiment_analysis_pt\\src\\saved_models\\embeddings\\w2v_embeddings_tiktok_sentences.model\n"
     ]
    }
   ],
   "source": [
    "# tokenizar las oraciones\n",
    "\n",
    "docs = tokenize(df_sentences['text'])\n",
    "create_w2v_model(\n",
    "    docs,\n",
    "    DIM_EMBEDDINGS,\n",
    "    EPOCHS,\n",
    "    SG,\n",
    "    EMBEDDING_W2V_TIKTOK_SENTENCES_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TamaÃ±o del vocabulario: 13663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('xochimilco', 0.5379130244255066),\n",
       " ('cuajimalpa', 0.5163016319274902),\n",
       " ('ðŸ†', 0.42668938636779785),\n",
       " ('azcapotzalco', 0.39767393469810486),\n",
       " ('ðŸ’»', 0.36573657393455505),\n",
       " ('iztapalapa', 0.36111846566200256),\n",
       " ('cics', 0.360874742269516),\n",
       " ('ðŸ’š', 0.3582456409931183),\n",
       " ('ðŸŒŸ', 0.3580889403820038),\n",
       " ('âš¡', 0.3579041063785553)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar modelo\n",
    "w2v = get_model(EMBEDDING_W2V_TIKTOK_SENTENCES_PATH)\n",
    "\n",
    "vocabulario = list(w2v.wv.index_to_key)\n",
    "print(f\" TamaÃ±o del vocabulario: {len(vocabulario)}\")\n",
    "\n",
    "w2v.wv.most_similar(\"uam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feo', 0.34824392199516296),\n",
       " ('intro', 0.3223033547401428),\n",
       " ('refleja', 0.31791478395462036),\n",
       " ('emocionada', 0.31210586428642273),\n",
       " ('encantando', 0.3111438453197479),\n",
       " ('baglass', 0.30976420640945435),\n",
       " ('infravalorada', 0.3083699941635132),\n",
       " ('preocupada', 0.30794647336006165),\n",
       " ('aceptando', 0.30201923847198486),\n",
       " ('acostumbrada', 0.3002161681652069)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(\"triste\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
